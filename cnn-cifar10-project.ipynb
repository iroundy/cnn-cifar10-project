{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is my science fair project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_dir='cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the environment\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a batch into memory\n",
    "def load_batch(data_dir, batch_id):\n",
    "    with open(os.path.join(cifar10_dir, 'data_batch_%i' % batch_id), mode='rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "    feats = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    lbls = batch['labels']\n",
    "    return feats, lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and load the first batch\n",
    "feats, labels = load_batch(cifar10_dir, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Id: 7 - Class: horse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1240ff240>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_id = 9505\n",
    "sample_img = feats[sample_id]\n",
    "sample_lbl = labels[sample_id]\n",
    "print('Label Id: {} - Class: {}'.format(sample_lbl, label_names[sample_lbl]))\n",
    "plt.imshow(sample_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 254, (32, 32, 3)]\n"
     ]
    }
   ],
   "source": [
    "# some stats\n",
    "print([sample_img.min(), sample_img.max(), sample_img.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return x / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer().fit(range(10))\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    global lb\n",
    "    return lb.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feats, test_lbls = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    feats, lbls = load_batch(cifar10_dir, i)\n",
    "    test_size = int(len(feats) * 0.1)\n",
    "    \n",
    "    # training data\n",
    "    norm_feats = normalize(feats[:-test_size])\n",
    "    one_hot_lbls = one_hot_encode(lbls[:-test_size])\n",
    "    pickle.dump((norm_feats, one_hot_lbls), open('preprocess_batch_%i.p' % i, 'wb'))\n",
    "    \n",
    "    # add the rest to the test data\n",
    "    test_feats.extend(feats[-test_size:])\n",
    "    test_lbls.extend(lbls[-test_size:])\n",
    "\n",
    "# dump the test data too\n",
    "norm_test_feats = normalize(np.array(test_feats))\n",
    "one_hot_test_lbls = one_hot_encode(np.array(test_lbls))\n",
    "pickle.dump((norm_test_feats, one_hot_test_lbls), open('preprocess_test.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # Weights\n",
    "    W_shape = list(conv_ksize) + [int(x_tensor.shape[3]), conv_num_outputs]\n",
    "    W = tf.Variable(tf.truncated_normal(W_shape, stddev=.05))\n",
    "    \n",
    "    # Apply convolution\n",
    "    x = tf.nn.conv2d(\n",
    "        x_tensor, W,\n",
    "        strides = [1] + list(conv_strides) + [1],\n",
    "        padding = 'SAME'\n",
    "    )\n",
    "    \n",
    "    # Add bias\n",
    "    b = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    \n",
    "    # Nonlinear activation (ReLU)\n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    # Max pooling\n",
    "    return tf.nn.m pool(\n",
    "        x,\n",
    "        ksize = [1] + list(pool_ksize) + [1],\n",
    "        strides = [1] + list(pool_strides) + [1],\n",
    "        padding = 'SAME'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    return tf.reshape(x_tensor, [-1, np.prod(x_tensor.shape.as_list()[1:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # Weights and bias\n",
    "    W = tf.Variable(tf.truncated_normal([int(x_tensor.shape[1]), num_outputs], stddev=.05))\n",
    "    b = tf.Variable(tf.zeros([num_outputs]))\n",
    "    \n",
    "    # The fully connected layer\n",
    "    x = tf.add(tf.matmul(x_tensor, W), b)\n",
    "    \n",
    "    # ReLU activation function\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # Weights and bias\n",
    "    W = tf.Variable(tf.truncated_normal([int(x_tensor.shape[1]), num_outputs], stddev=.05))\n",
    "    b = tf.Variable(tf.zeros([num_outputs]))\n",
    "    \n",
    "    # The output layer\n",
    "    return tf.add(tf.matmul(x_tensor, W), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # 3 convolution layers with max pooling\n",
    "    # All layers with same kernel, stride and maxpooling params\n",
    "    x = conv2d_maxpool(x, 64, (3,3), (1,1), (2,2), (2,2))\n",
    "    x = conv2d_maxpool(x, 128, (3,3), (1,1), (2,2), (2,2))\n",
    "    x = conv2d_maxpool(x, 256, (3,3), (1,1), (2,2), (2,2))\n",
    "    \n",
    "    # dropout after convolutions\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # flatten layer\n",
    "    x = flatten(x)\n",
    "\n",
    "    # 1 fully connected layer followed by dropout\n",
    "    x = fully_conn(x, 1024)\n",
    "    x = tf.nn.dropout(x, keep_prob)\n",
    "    \n",
    "    # output layer\n",
    "    return output(x, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-879dbd7a7fd9>:16: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3], name=\"x\")\n",
    "y = tf.placeholder(tf.float32, [None, 10], name=\"y\")\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    global valid_features, valid_labels\n",
    "    loss = sess.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    valid_acc = sess.run(\n",
    "        accuracy, \n",
    "        feed_dict={\n",
    "            x: norm_test_feats,\n",
    "            y: one_hot_test_lbls,\n",
    "            keep_prob: 1.\n",
    "        }\n",
    "    )\n",
    "    print('Loss: {:>8.4f}, Validation Accuracy: {:>8.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 80\n",
    "batch_size = 1024\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]\n",
    "\n",
    "\n",
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
    "    features, labels = pickle.load(open(filename, mode='rb'))\n",
    "\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    return batch_features_labels(features, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:   2.1852, Validation Accuracy: 0.238400\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:   1.9302, Validation Accuracy: 0.324000\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:   1.8220, Validation Accuracy: 0.331200\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:   1.7094, Validation Accuracy: 0.362200\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:   1.5995, Validation Accuracy: 0.417400\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:   1.5199, Validation Accuracy: 0.443400\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:   1.4819, Validation Accuracy: 0.464200\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:   1.3456, Validation Accuracy: 0.482800\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:   1.3631, Validation Accuracy: 0.502000\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:   1.2991, Validation Accuracy: 0.514400\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:   1.2870, Validation Accuracy: 0.528600\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:   1.3077, Validation Accuracy: 0.514400\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:   1.1733, Validation Accuracy: 0.546800\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:   1.1619, Validation Accuracy: 0.560400\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:   1.1256, Validation Accuracy: 0.574400\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:   1.1654, Validation Accuracy: 0.559400\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:   1.1406, Validation Accuracy: 0.591400\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:   1.0783, Validation Accuracy: 0.577000\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:   1.0346, Validation Accuracy: 0.604400\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:   0.9836, Validation Accuracy: 0.614400\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:   1.0059, Validation Accuracy: 0.627200\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:   0.9911, Validation Accuracy: 0.635200\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:   0.9531, Validation Accuracy: 0.617600\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:   0.9370, Validation Accuracy: 0.630000\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:   0.8815, Validation Accuracy: 0.653400\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:   0.9441, Validation Accuracy: 0.631600\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:   0.9098, Validation Accuracy: 0.655000\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:   0.8776, Validation Accuracy: 0.651200\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:   0.8232, Validation Accuracy: 0.666200\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:   0.8018, Validation Accuracy: 0.670200\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:   0.8492, Validation Accuracy: 0.673000\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:   0.8668, Validation Accuracy: 0.665600\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:   0.7766, Validation Accuracy: 0.687600\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:   0.7467, Validation Accuracy: 0.691200\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:   0.7201, Validation Accuracy: 0.697200\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:   0.7861, Validation Accuracy: 0.689400\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:   0.8612, Validation Accuracy: 0.662400\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:   0.7272, Validation Accuracy: 0.702000\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:   0.7073, Validation Accuracy: 0.706200\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:   0.6810, Validation Accuracy: 0.708600\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:   0.7224, Validation Accuracy: 0.705000\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:   0.7023, Validation Accuracy: 0.713200\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:   0.6947, Validation Accuracy: 0.709000\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:   0.6426, Validation Accuracy: 0.718200\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:   0.6338, Validation Accuracy: 0.718800\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:   0.6826, Validation Accuracy: 0.723600\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:   0.6601, Validation Accuracy: 0.724600\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:   0.6472, Validation Accuracy: 0.719400\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:   0.6115, Validation Accuracy: 0.723400\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:   0.5832, Validation Accuracy: 0.726000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:   0.6138, Validation Accuracy: 0.739800\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:   0.6052, Validation Accuracy: 0.738200\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:   0.5864, Validation Accuracy: 0.737800\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:   0.5577, Validation Accuracy: 0.744400\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:   0.5318, Validation Accuracy: 0.740400\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:   0.5811, Validation Accuracy: 0.742000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:   0.5649, Validation Accuracy: 0.745200\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:   0.5572, Validation Accuracy: 0.740800\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:   0.5314, Validation Accuracy: 0.753400\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:   0.4937, Validation Accuracy: 0.754000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:   0.5409, Validation Accuracy: 0.760400\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:   0.5225, Validation Accuracy: 0.752800\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:   0.5436, Validation Accuracy: 0.735000\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:   0.4996, Validation Accuracy: 0.759400\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:   0.4711, Validation Accuracy: 0.753800\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:   0.4967, Validation Accuracy: 0.760800\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:   0.4957, Validation Accuracy: 0.753600\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:   0.4963, Validation Accuracy: 0.751400\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:   0.4646, Validation Accuracy: 0.766200\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:   0.4249, Validation Accuracy: 0.765200\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:   0.4781, Validation Accuracy: 0.766200\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:   0.4501, Validation Accuracy: 0.768600\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:   0.4450, Validation Accuracy: 0.762200\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:   0.4515, Validation Accuracy: 0.760600\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:   0.4082, Validation Accuracy: 0.765200\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:   0.4550, Validation Accuracy: 0.761200\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:   0.4258, Validation Accuracy: 0.777200\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:   0.4592, Validation Accuracy: 0.751000\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:   0.4603, Validation Accuracy: 0.747400\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:   0.3973, Validation Accuracy: 0.760800\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:   0.4136, Validation Accuracy: 0.778600\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:   0.4091, Validation Accuracy: 0.770200\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:   0.3839, Validation Accuracy: 0.767800\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:   0.3973, Validation Accuracy: 0.767200\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:   0.3569, Validation Accuracy: 0.766000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:   0.4125, Validation Accuracy: 0.769800\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:   0.3723, Validation Accuracy: 0.776400\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:   0.3750, Validation Accuracy: 0.769800\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:   0.3496, Validation Accuracy: 0.778200\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:   0.3193, Validation Accuracy: 0.773600\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:   0.3885, Validation Accuracy: 0.775000\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:   0.3434, Validation Accuracy: 0.773800\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:   0.3295, Validation Accuracy: 0.788800\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:   0.3399, Validation Accuracy: 0.773600\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:   0.3161, Validation Accuracy: 0.771000\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:   0.3437, Validation Accuracy: 0.774800\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:   0.3216, Validation Accuracy: 0.778200\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:   0.3010, Validation Accuracy: 0.787600\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:   0.3164, Validation Accuracy: 0.784200\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:   0.2683, Validation Accuracy: 0.790000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:   0.3367, Validation Accuracy: 0.779800\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:   0.2998, Validation Accuracy: 0.788200\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:   0.3039, Validation Accuracy: 0.784800\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:   0.2995, Validation Accuracy: 0.776400\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:   0.2520, Validation Accuracy: 0.785600\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:   0.3096, Validation Accuracy: 0.789800\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:   0.2686, Validation Accuracy: 0.787800\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:   0.2725, Validation Accuracy: 0.787800\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:   0.2728, Validation Accuracy: 0.786400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, CIFAR-10 Batch 5:  Loss:   0.2565, Validation Accuracy: 0.773400\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:   0.3031, Validation Accuracy: 0.781200\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:   0.2516, Validation Accuracy: 0.782200\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:   0.2411, Validation Accuracy: 0.790000\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:   0.2871, Validation Accuracy: 0.774000\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:   0.2487, Validation Accuracy: 0.782200\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:   0.2852, Validation Accuracy: 0.785000\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:   0.2618, Validation Accuracy: 0.779200\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:   0.2239, Validation Accuracy: 0.787800\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:   0.2246, Validation Accuracy: 0.794600\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:   0.1912, Validation Accuracy: 0.793600\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:   0.2444, Validation Accuracy: 0.791800\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:   0.2272, Validation Accuracy: 0.787800\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:   0.2030, Validation Accuracy: 0.791600\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:   0.2096, Validation Accuracy: 0.792800\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:   0.1818, Validation Accuracy: 0.786200\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:   0.2492, Validation Accuracy: 0.786000\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:   0.2137, Validation Accuracy: 0.795600\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:   0.2012, Validation Accuracy: 0.794800\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:   0.2041, Validation Accuracy: 0.789400\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:   0.1838, Validation Accuracy: 0.784800\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:   0.2141, Validation Accuracy: 0.788400\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:   0.1995, Validation Accuracy: 0.800600\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:   0.1865, Validation Accuracy: 0.785800\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:   0.2032, Validation Accuracy: 0.783800\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:   0.1622, Validation Accuracy: 0.789800\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:   0.1955, Validation Accuracy: 0.792800\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:   0.1926, Validation Accuracy: 0.800200\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:   0.1790, Validation Accuracy: 0.789800\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:   0.1708, Validation Accuracy: 0.795800\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:   0.1362, Validation Accuracy: 0.792400\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:   0.1865, Validation Accuracy: 0.796400\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:   0.1748, Validation Accuracy: 0.798800\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:   0.1511, Validation Accuracy: 0.795000\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:   0.1544, Validation Accuracy: 0.798800\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:   0.1296, Validation Accuracy: 0.799400\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:   0.1889, Validation Accuracy: 0.789600\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:   0.1642, Validation Accuracy: 0.796200\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:   0.1448, Validation Accuracy: 0.804000\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:   0.1446, Validation Accuracy: 0.806400\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:   0.1248, Validation Accuracy: 0.796200\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:   0.1592, Validation Accuracy: 0.794000\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:   0.1493, Validation Accuracy: 0.798400\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:   0.1324, Validation Accuracy: 0.797800\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:   0.1212, Validation Accuracy: 0.809200\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:   0.1101, Validation Accuracy: 0.797800\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:   0.1391, Validation Accuracy: 0.803800\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:   0.1371, Validation Accuracy: 0.793600\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:   0.1263, Validation Accuracy: 0.805000\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:   0.1143, Validation Accuracy: 0.806200\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:   0.0979, Validation Accuracy: 0.806600\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:   0.1281, Validation Accuracy: 0.803000\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:   0.0991, Validation Accuracy: 0.804000\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:   0.1149, Validation Accuracy: 0.805400\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:   0.1177, Validation Accuracy: 0.795000\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:   0.0910, Validation Accuracy: 0.803200\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:   0.1334, Validation Accuracy: 0.800600\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:   0.0903, Validation Accuracy: 0.807200\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:   0.1187, Validation Accuracy: 0.793400\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:   0.1000, Validation Accuracy: 0.802600\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:   0.0860, Validation Accuracy: 0.805000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:   0.1022, Validation Accuracy: 0.807200\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:   0.0935, Validation Accuracy: 0.807400\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:   0.0959, Validation Accuracy: 0.805000\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:   0.0917, Validation Accuracy: 0.807800\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:   0.0666, Validation Accuracy: 0.810000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:   0.0919, Validation Accuracy: 0.806200\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:   0.0780, Validation Accuracy: 0.808000\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:   0.0951, Validation Accuracy: 0.806800\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:   0.0883, Validation Accuracy: 0.806000\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:   0.0733, Validation Accuracy: 0.807400\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:   0.0875, Validation Accuracy: 0.810000\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:   0.0867, Validation Accuracy: 0.806000\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:   0.0854, Validation Accuracy: 0.805600\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:   0.0738, Validation Accuracy: 0.802400\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:   0.0624, Validation Accuracy: 0.800800\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:   0.0800, Validation Accuracy: 0.805400\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:   0.0813, Validation Accuracy: 0.805800\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:   0.0716, Validation Accuracy: 0.815600\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:   0.0790, Validation Accuracy: 0.793800\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:   0.0689, Validation Accuracy: 0.797800\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:   0.0844, Validation Accuracy: 0.798600\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:   0.0716, Validation Accuracy: 0.804600\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:   0.0723, Validation Accuracy: 0.806000\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:   0.0703, Validation Accuracy: 0.806800\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:   0.0631, Validation Accuracy: 0.803400\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:   0.0841, Validation Accuracy: 0.803400\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:   0.0631, Validation Accuracy: 0.807000\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:   0.0717, Validation Accuracy: 0.814800\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:   0.0615, Validation Accuracy: 0.805200\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:   0.0597, Validation Accuracy: 0.799800\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:   0.0719, Validation Accuracy: 0.799000\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:   0.0498, Validation Accuracy: 0.811000\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:   0.0626, Validation Accuracy: 0.808600\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:   0.0535, Validation Accuracy: 0.811200\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:   0.0441, Validation Accuracy: 0.812400\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:   0.0653, Validation Accuracy: 0.802400\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:   0.0473, Validation Accuracy: 0.813400\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:   0.0638, Validation Accuracy: 0.814600\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:   0.0550, Validation Accuracy: 0.803400\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:   0.0512, Validation Accuracy: 0.802400\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:   0.0598, Validation Accuracy: 0.805200\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:   0.0553, Validation Accuracy: 0.805200\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:   0.0579, Validation Accuracy: 0.812200\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:   0.0483, Validation Accuracy: 0.812600\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:   0.0404, Validation Accuracy: 0.809600\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:   0.0497, Validation Accuracy: 0.804600\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:   0.0462, Validation Accuracy: 0.801200\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:   0.0534, Validation Accuracy: 0.814600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, CIFAR-10 Batch 4:  Loss:   0.0413, Validation Accuracy: 0.814400\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:   0.0357, Validation Accuracy: 0.815200\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:   0.0419, Validation Accuracy: 0.811400\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:   0.0369, Validation Accuracy: 0.805200\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:   0.0523, Validation Accuracy: 0.815600\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:   0.0368, Validation Accuracy: 0.817800\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:   0.0326, Validation Accuracy: 0.815600\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:   0.0393, Validation Accuracy: 0.814200\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:   0.0389, Validation Accuracy: 0.803000\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:   0.0416, Validation Accuracy: 0.818200\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:   0.0388, Validation Accuracy: 0.809400\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:   0.0291, Validation Accuracy: 0.812400\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:   0.0371, Validation Accuracy: 0.814000\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:   0.0323, Validation Accuracy: 0.808200\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:   0.0392, Validation Accuracy: 0.816400\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:   0.0421, Validation Accuracy: 0.805800\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:   0.0306, Validation Accuracy: 0.818800\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:   0.0345, Validation Accuracy: 0.815000\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:   0.0311, Validation Accuracy: 0.803800\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:   0.0436, Validation Accuracy: 0.810200\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:   0.0376, Validation Accuracy: 0.803400\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:   0.0277, Validation Accuracy: 0.813600\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:   0.0331, Validation Accuracy: 0.811400\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:   0.0276, Validation Accuracy: 0.809200\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:   0.0432, Validation Accuracy: 0.811400\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:   0.0361, Validation Accuracy: 0.812200\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:   0.0275, Validation Accuracy: 0.815200\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:   0.0293, Validation Accuracy: 0.808200\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:   0.0280, Validation Accuracy: 0.806800\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:   0.0321, Validation Accuracy: 0.809200\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:   0.0307, Validation Accuracy: 0.817400\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:   0.0257, Validation Accuracy: 0.809600\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:   0.0333, Validation Accuracy: 0.804000\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:   0.0291, Validation Accuracy: 0.815000\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:   0.0384, Validation Accuracy: 0.805000\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:   0.0296, Validation Accuracy: 0.813800\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:   0.0241, Validation Accuracy: 0.811600\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:   0.0207, Validation Accuracy: 0.816800\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:   0.0267, Validation Accuracy: 0.811800\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:   0.0299, Validation Accuracy: 0.807600\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:   0.0260, Validation Accuracy: 0.812400\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:   0.0236, Validation Accuracy: 0.808000\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:   0.0292, Validation Accuracy: 0.810800\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:   0.0209, Validation Accuracy: 0.822800\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:   0.0258, Validation Accuracy: 0.815400\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:   0.0209, Validation Accuracy: 0.811000\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:   0.0231, Validation Accuracy: 0.810400\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:   0.0207, Validation Accuracy: 0.818600\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:   0.0232, Validation Accuracy: 0.807000\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:   0.0224, Validation Accuracy: 0.819800\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:   0.0171, Validation Accuracy: 0.815400\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:   0.0198, Validation Accuracy: 0.817600\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:   0.0209, Validation Accuracy: 0.813000\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:   0.0217, Validation Accuracy: 0.809600\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:   0.0208, Validation Accuracy: 0.816800\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:   0.0156, Validation Accuracy: 0.816800\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:   0.0212, Validation Accuracy: 0.808600\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:   0.0253, Validation Accuracy: 0.807600\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:   0.0255, Validation Accuracy: 0.804000\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:   0.0236, Validation Accuracy: 0.814200\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:   0.0205, Validation Accuracy: 0.800600\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:   0.0212, Validation Accuracy: 0.809800\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:   0.0181, Validation Accuracy: 0.812800\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:   0.0119, Validation Accuracy: 0.818600\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:   0.0193, Validation Accuracy: 0.816800\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:   0.0168, Validation Accuracy: 0.814400\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:   0.0202, Validation Accuracy: 0.801200\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:   0.0226, Validation Accuracy: 0.814800\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:   0.0171, Validation Accuracy: 0.815000\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:   0.0247, Validation Accuracy: 0.810400\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:   0.0143, Validation Accuracy: 0.810600\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:   0.0155, Validation Accuracy: 0.812400\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:   0.0172, Validation Accuracy: 0.815400\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:   0.0099, Validation Accuracy: 0.824400\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:   0.0179, Validation Accuracy: 0.820400\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:   0.0113, Validation Accuracy: 0.821800\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:   0.0113, Validation Accuracy: 0.819200\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:   0.0148, Validation Accuracy: 0.809400\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:   0.0129, Validation Accuracy: 0.815400\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:   0.0151, Validation Accuracy: 0.817000\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:   0.0121, Validation Accuracy: 0.822800\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:   0.0155, Validation Accuracy: 0.817800\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:   0.0157, Validation Accuracy: 0.816200\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:   0.0126, Validation Accuracy: 0.812200\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:   0.0189, Validation Accuracy: 0.811200\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:   0.0135, Validation Accuracy: 0.814600\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:   0.0104, Validation Accuracy: 0.821400\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:   0.0139, Validation Accuracy: 0.810800\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:   0.0097, Validation Accuracy: 0.812400\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:   0.0133, Validation Accuracy: 0.815400\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:   0.0083, Validation Accuracy: 0.824000\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:   0.0115, Validation Accuracy: 0.819000\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:   0.0105, Validation Accuracy: 0.820800\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:   0.0086, Validation Accuracy: 0.817000\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:   0.0121, Validation Accuracy: 0.817600\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:   0.0091, Validation Accuracy: 0.817000\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:   0.0086, Validation Accuracy: 0.821000\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:   0.0096, Validation Accuracy: 0.816200\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:   0.0086, Validation Accuracy: 0.813600\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:   0.0112, Validation Accuracy: 0.815200\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:   0.0098, Validation Accuracy: 0.819400\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:   0.0071, Validation Accuracy: 0.820600\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:   0.0088, Validation Accuracy: 0.820400\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:   0.0068, Validation Accuracy: 0.814600\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:   0.0103, Validation Accuracy: 0.817800\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:   0.0073, Validation Accuracy: 0.823400\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:   0.0076, Validation Accuracy: 0.817800\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:   0.0081, Validation Accuracy: 0.808000\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:   0.0074, Validation Accuracy: 0.809800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, CIFAR-10 Batch 3:  Loss:   0.0091, Validation Accuracy: 0.815800\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:   0.0070, Validation Accuracy: 0.823200\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:   0.0092, Validation Accuracy: 0.811800\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:   0.0070, Validation Accuracy: 0.819400\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:   0.0084, Validation Accuracy: 0.810200\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:   0.0080, Validation Accuracy: 0.819000\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:   0.0068, Validation Accuracy: 0.817200\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:   0.0068, Validation Accuracy: 0.821800\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:   0.0071, Validation Accuracy: 0.814600\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:   0.0083, Validation Accuracy: 0.812600\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:   0.0081, Validation Accuracy: 0.821200\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:   0.0061, Validation Accuracy: 0.820400\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:   0.0053, Validation Accuracy: 0.819000\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:   0.0061, Validation Accuracy: 0.821600\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:   0.0053, Validation Accuracy: 0.811600\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:   0.0075, Validation Accuracy: 0.819000\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:   0.0065, Validation Accuracy: 0.826800\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:   0.0051, Validation Accuracy: 0.821600\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:   0.0065, Validation Accuracy: 0.816000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:   0.0060, Validation Accuracy: 0.809000\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:   0.0098, Validation Accuracy: 0.817400\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:   0.0094, Validation Accuracy: 0.814000\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:   0.0065, Validation Accuracy: 0.814600\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:   0.0074, Validation Accuracy: 0.815000\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:   0.0077, Validation Accuracy: 0.813800\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:   0.0130, Validation Accuracy: 0.807600\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:   0.0089, Validation Accuracy: 0.815400\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:   0.0051, Validation Accuracy: 0.821800\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:   0.0095, Validation Accuracy: 0.817000\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:   0.0060, Validation Accuracy: 0.820400\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:   0.0099, Validation Accuracy: 0.812000\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:   0.0154, Validation Accuracy: 0.808800\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:   0.0077, Validation Accuracy: 0.811600\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:   0.0063, Validation Accuracy: 0.820400\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:   0.0077, Validation Accuracy: 0.813000\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:   0.0093, Validation Accuracy: 0.816000\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:   0.0087, Validation Accuracy: 0.813200\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:   0.0051, Validation Accuracy: 0.822000\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:   0.0060, Validation Accuracy: 0.819800\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:   0.0049, Validation Accuracy: 0.817600\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:   0.0088, Validation Accuracy: 0.816400\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:   0.0080, Validation Accuracy: 0.812200\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:   0.0055, Validation Accuracy: 0.818600\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:   0.0080, Validation Accuracy: 0.815800\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:   0.0050, Validation Accuracy: 0.817200\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:   0.0094, Validation Accuracy: 0.821800\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:   0.0062, Validation Accuracy: 0.820400\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:   0.0059, Validation Accuracy: 0.814000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:   0.0073, Validation Accuracy: 0.815800\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:   0.0079, Validation Accuracy: 0.809800\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:   0.0083, Validation Accuracy: 0.820600\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:   0.0054, Validation Accuracy: 0.820600\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:   0.0051, Validation Accuracy: 0.823600\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:   0.0061, Validation Accuracy: 0.817000\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:   0.0069, Validation Accuracy: 0.811000\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:   0.0092, Validation Accuracy: 0.818400\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:   0.0058, Validation Accuracy: 0.821400\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:   0.0049, Validation Accuracy: 0.821200\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:   0.0046, Validation Accuracy: 0.817200\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:   0.0049, Validation Accuracy: 0.811800\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:   0.0062, Validation Accuracy: 0.814400\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:   0.0057, Validation Accuracy: 0.818400\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:   0.0041, Validation Accuracy: 0.820800\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:   0.0045, Validation Accuracy: 0.818200\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:   0.0043, Validation Accuracy: 0.819200\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:   0.0076, Validation Accuracy: 0.817800\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:   0.0054, Validation Accuracy: 0.822800\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:   0.0063, Validation Accuracy: 0.820200\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:   0.0054, Validation Accuracy: 0.817200\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:   0.0068, Validation Accuracy: 0.817200\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:   0.0066, Validation Accuracy: 0.819800\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:   0.0043, Validation Accuracy: 0.824200\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:   0.0036, Validation Accuracy: 0.825600\n"
     ]
    }
   ],
   "source": [
    "print(\"Training...\")\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches+1):\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
    "                sess.run(\n",
    "                    optimizer, \n",
    "                    feed_dict={\n",
    "                        x: batch_features, \n",
    "                        y: batch_labels, \n",
    "                        keep_prob: keep_probability\n",
    "                    }\n",
    "                )\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "    \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, './image_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './image_classification'\n",
    "n_sample = 4\n",
    "top_n_predictions = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_labels = pickle.load(open('preprocessing_test.p', mode='rb'))\n",
    "loaded_graph = tf.Graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image_predictions(features, labels, predictions):\n",
    "    n_classes = 10\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(range(n_classes))\n",
    "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
    "\n",
    "    fig, axies = plt.subplots(nrows=4, ncols=2)\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
    "\n",
    "    n_predictions = 3\n",
    "    margin = 0.05\n",
    "    ind = np.arange(n_predictions)\n",
    "    width = (1. - 2. * margin) / n_predictions\n",
    "\n",
    "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n",
    "        pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
    "        correct_name = label_names[label_id]\n",
    "\n",
    "        axies[image_i][0].imshow(feature)\n",
    "        axies[image_i][0].set_title(correct_name)\n",
    "        axies[image_i][0].set_axis_off()\n",
    "\n",
    "        axies[image_i][1].barh(ind + margin, pred_values[::-1], width)\n",
    "        axies[image_i][1].set_yticks(ind + margin)\n",
    "        axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
    "        axies[image_i][1].set_xticks([0, 0.5, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(model_path + '.meta')\n",
    "    loader.restore(sess, model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "    loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "    loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "    loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "    # Get accuracy in batches for memory limitations\n",
    "    test_batch_acc_total = 0\n",
    "    test_batch_count = 0\n",
    "    \n",
    "    for test_feature_batch, test_label_batch in batch_features_labels(test_features, test_labels, batch_size):\n",
    "        test_batch_acc_total += sess.run(\n",
    "            loaded_acc,\n",
    "            feed_dict={\n",
    "                loaded_x: test_feature_batch, \n",
    "                loaded_y: test_label_batch, \n",
    "                loaded_keep_prob: 1.0\n",
    "            }\n",
    "        )\n",
    "        test_batch_count += 1\n",
    "\n",
    "    print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "    # Print Random Samples\n",
    "    random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "    random_test_predictions = sess.run(\n",
    "        tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "        feed_dict={\n",
    "            loaded_x: random_test_features, \n",
    "            loaded_y: random_test_labels, \n",
    "            loaded_keep_prob: 1.0\n",
    "        }\n",
    "    )\n",
    "    display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
